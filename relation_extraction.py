#!/usr/bin/env python3
"""
åŸºäºNERæ¨¡å‹çš„å…³ç³»æŠ½å–ç³»ç»Ÿ
ä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨è§„åˆ™å’Œæ¨¡å¼åŒ¹é…è¿›è¡Œå…³ç³»æŠ½å–
åŸºäºå·²è®­ç»ƒçš„BERT-NER-CRFæ¨¡å‹
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import re
import numpy as np
from collections import defaultdict
from transformers import BertConfig, BertTokenizer
from models.bert_for_ner import BertCrfForNer
from processors.ner_seq import convert_examples_to_features, CnerProcessor
from processors.utils_ner import get_entities
from processors.ner_seq import InputExample

# å¯¼å…¥ safetensors
try:
    from safetensors.torch import load_file
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False
    print("è­¦å‘Š: safetensors æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install safetensors")

class RelationExtractor:
    def __init__(self, model_path):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.device = None
        self.processor = None
        self.label_list = None
        self.id2label = None
        
        # å®ä½“ç±»å‹æ˜ å°„
        self.entity_map = {
            'CONT': 'è”ç³»æ–¹å¼', 'EDU': 'æ•™è‚²èƒŒæ™¯', 'LOC': 'åœ°å€',
            'NAME': 'å§“å', 'ORG': 'ç»„ç»‡æœºæ„', 'PRO': 'ä¸“ä¸š',
            'RACE': 'æ°‘æ—', 'TITLE': 'èŒä½'
        }
        
        # å®šä¹‰å…³ç³»æ¨¡å¼å’Œè§„åˆ™
        self.relation_patterns = self._init_relation_patterns()
        
        self.load_model()
    
    def _init_relation_patterns(self):
        """åˆå§‹åŒ–å…³ç³»æŠ½å–çš„æ¨¡å¼å’Œè§„åˆ™"""
        patterns = {
            # äººå‘˜-ç»„ç»‡å…³ç³» (æ–¹å‘: NAME -> ORG)
            "å·¥ä½œäº": {
                "subject_type": "NAME",
                "object_type": "ORG",
                "patterns": [
                    r"åœ¨(.+?)(?:å·¥ä½œ|ä»»èŒ|å°±èŒ|æœåŠ¡)",
                    r"(?:å·¥ä½œ|ä»»èŒ|å°±èŒ|æœåŠ¡)äº(.+?)",
                    r"(.+?)(?:å…¬å¸|é›†å›¢|ä¼ä¸š|æœºæ„)(?:çš„)?(?:å‘˜å·¥|èŒå‘˜)",
                    r"(?:åŠ å…¥|è¿›å…¥|å…¥èŒ)(.+?)"
                ],
                "keywords": ["å·¥ä½œ", "ä»»èŒ", "æœåŠ¡", "å°±èŒ", "å‘˜å·¥", "èŒå‘˜", "å…¥èŒ", "åŠ å…¥", "å…¬å¸", "ä¼ä¸š"],
                "distance_weight": 0.2,
                "syntax_patterns": [
                    {"pattern": r"(.+?)åœ¨(.+?)(?:å·¥ä½œ|ä»»èŒ)", "confidence": 0.9},
                    {"pattern": r"(.+?)(?:å·¥ä½œ|ä»»èŒ)äº(.+?)", "confidence": 0.85}
                ]
            },
            
            # äººå‘˜-èŒä½å…³ç³» (æ–¹å‘: NAME -> TITLE)
            "æ‹…ä»»": {
                "subject_type": "NAME",
                "object_type": "TITLE",
                "patterns": [
                    r"(?:æ‹…ä»»|å‡ºä»»|ä»»å‘½ä¸º|å‡ä»»)(.+?)(?:èŒä½|å²—ä½|ä¸€èŒ)?",
                    r"æ˜¯(.+?)(?:èŒä½|å²—ä½)?(?:ï¼Œ|ã€‚|$)",  # é¿å…ä¸"æ¯•ä¸šäº"å†²çª
                    r"(?:èŒåŠ¡|èŒä½)æ˜¯(.+?)(?:ï¼Œ|ã€‚|$)",
                    r"(?:å½“|åš|å¹²)(.+?)(?:å·¥ä½œ|èŒä½)?(?:ï¼Œ|ã€‚|$)",
                    r"ç°(?:ä»»|åœ¨)(.+?)(?:ï¼Œ|ã€‚|$)"  # å¼ºè°ƒç°åœ¨çš„èŒä½
                ],
                "keywords": ["æ‹…ä»»", "æ˜¯", "ä»»", "å½“", "åš", "èŒä½", "èŒåŠ¡", "å²—ä½", "å‡ºä»»", "å‡ä»»", "ç°ä»»"],
                "distance_weight": 0.25,
                "syntax_patterns": [
                    {"pattern": r"(.+?)æ‹…ä»»(.+?)", "confidence": 0.95},
                    {"pattern": r"(.+?)ç°(?:ä»»|åœ¨)(.+?)(?:èŒä½|å²—ä½|èŒåŠ¡)", "confidence": 0.9},
                    {"pattern": r"(.+?)æ˜¯(.+?)(?:èŒä½|å²—ä½|èŒåŠ¡)", "confidence": 0.8}
                ]
            },
            
            # äººå‘˜-åœ°å€å…³ç³» (æ–¹å‘: NAME -> LOC)
            "å±…ä½äº": {
                "subject_type": "NAME",
                "object_type": "LOC",
                "patterns": [
                    r"(?:ä½åœ¨|å±…ä½åœ¨|ç”Ÿæ´»åœ¨)(.+?)",
                    r"å®¶(?:ä½|åœ¨)(.+?)",
                    r"(?:æ¥è‡ª|å‡ºç”Ÿäº)(.+?)",
                    r"(?:æˆ·ç±|ç±è´¯)(?:åœ¨|æ˜¯)(.+?)",
                    r"ç°ä½å€(?:åœ¨|æ˜¯|ä¸º)(.+?)"
                ],
                "keywords": ["ä½", "å±…ä½", "å®¶", "æ¥è‡ª", "å‡ºç”Ÿ", "æˆ·ç±", "ç±è´¯", "ä½å€", "ç”Ÿæ´»"],
                "distance_weight": 0.2,
                "syntax_patterns": [
                    {"pattern": r"(.+?)ä½åœ¨(.+?)", "confidence": 0.9},
                    {"pattern": r"(.+?)æ¥è‡ª(.+?)", "confidence": 0.85}
                ]
            },
            
            # äººå‘˜-è”ç³»æ–¹å¼å…³ç³» (æ–¹å‘: NAME -> CONT)
            "è”ç³»æ–¹å¼": {
                "subject_type": "NAME",
                "object_type": "CONT",
                "patterns": [
                    r"(?:ç”µè¯|æ‰‹æœº|è”ç³»æ–¹å¼|è”ç³»ç”µè¯)(?:æ˜¯|ä¸º|:)(.+?)",
                    r"(?:é‚®ç®±|é‚®ä»¶|email)(?:æ˜¯|ä¸º|:)(.+?)",
                    r"(?:å¾®ä¿¡|QQ|WeChat)(?:å·|æ˜¯|ä¸º|:)(.+?)",
                    r"(?:æ‰‹æœºå·ç |ç”µè¯å·ç )(?:æ˜¯|ä¸º|:)(.+?)"
                ],
                "keywords": ["ç”µè¯", "æ‰‹æœº", "è”ç³»", "é‚®ç®±", "å¾®ä¿¡", "QQ", "å·ç ", "email"],
                "distance_weight": 0.1,
                "syntax_patterns": [
                    {"pattern": r"(.+?)(?:çš„)?(?:ç”µè¯|æ‰‹æœº)(?:æ˜¯|ä¸º)(.+?)", "confidence": 0.95}
                ]
            },
            
            # äººå‘˜-æ•™è‚²èƒŒæ™¯å…³ç³» (æ–¹å‘: NAME -> EDU)
            "æ¯•ä¸šäº": {
                "subject_type": "NAME",
                "object_type": "EDU", 
                "patterns": [
                    r"(?:æ¯•ä¸šäº|æ¯•ä¸šè‡ª)(.+?)(?:å¤§å­¦|å­¦é™¢|å­¦æ ¡|é«˜ä¸­|ä¸­å­¦)",
                    r"(?:å°±è¯»äº|åœ¨è¯»äº)(.+?)(?:å¤§å­¦|å­¦é™¢|å­¦æ ¡|é«˜ä¸­|ä¸­å­¦)",
                    r"åœ¨(.+?)(?:å¤§å­¦|å­¦é™¢|å­¦æ ¡|é«˜ä¸­|ä¸­å­¦)(?:å­¦ä¹ |å°±è¯»|æ¯•ä¸š)",
                    r"(?:å­¦å†|æœ€é«˜å­¦å†)(?:æ˜¯|ä¸º)(.+?)",
                    r"(?:è·å¾—|å–å¾—)(.+?)(?:å­¦ä½|æ–‡å‡­|æ¯•ä¸šè¯)",
                    r"(.+?)(?:å¤§å­¦|å­¦é™¢|å­¦æ ¡|é«˜ä¸­|ä¸­å­¦)æ¯•ä¸š"
                ],
                "keywords": ["æ¯•ä¸š", "å°±è¯»", "å­¦ä¹ ", "å­¦å†", "å­¦ä½", "å¤§å­¦", "å­¦é™¢", "å­¦æ ¡", "æ–‡å‡­", "æ¯•ä¸šè¯"],
                "distance_weight": 0.3,
                "syntax_patterns": [
                    {"pattern": r"(.+?)æ¯•ä¸šäº(.+?)(?:å¤§å­¦|å­¦é™¢|å­¦æ ¡)", "confidence": 0.98},
                    {"pattern": r"(.+?)åœ¨(.+?)(?:å¤§å­¦|å­¦é™¢|å­¦æ ¡)(?:å­¦ä¹ |å°±è¯»|æ¯•ä¸š)", "confidence": 0.92}
                ]
            },
            
            # äººå‘˜-ä¸“ä¸šå…³ç³» (æ–¹å‘: NAME -> PRO)
            "ä¸“ä¸šæ˜¯": {
                "subject_type": "NAME",
                "object_type": "PRO",
                "patterns": [
                    r"(?:ä¸“ä¸šæ˜¯|ä¸“ä¸šä¸º|å­¦çš„æ˜¯)(.+?)",
                    r"(?:ä¸»ä¿®|æ‰€å­¦ä¸“ä¸šæ˜¯)(.+?)",
                    r"(?:ç ”ç©¶æ–¹å‘|ç ”ç©¶é¢†åŸŸ)(?:æ˜¯|ä¸º)(.+?)",
                    r"(?:ä»äº‹|ä¸“é—¨ä»äº‹)(.+?)(?:ç›¸å…³|æ–¹é¢)(?:å·¥ä½œ|ç ”ç©¶)?",
                    r"(.+?)ä¸“ä¸š(?:æ¯•ä¸š|å‡ºèº«)"
                ],
                "keywords": ["ä¸“ä¸š", "ä¸»ä¿®", "ç ”ç©¶", "ä»äº‹", "å­¦çš„", "æ–¹å‘", "é¢†åŸŸ"],
                "distance_weight": 0.2,
                "syntax_patterns": [
                    {"pattern": r"(.+?)(?:çš„)?ä¸“ä¸šæ˜¯(.+?)", "confidence": 0.9}
                ]
            },
            
            # ç»„ç»‡-åœ°å€å…³ç³» (æ–¹å‘: ORG -> LOC)
            "ä½äº": {
                "subject_type": "ORG",
                "object_type": "LOC",
                "patterns": [
                    r"(?:ä½äº|åè½äº|è®¾åœ¨|å»ºåœ¨)(.+?)",
                    r"åœ¨(.+?)(?:è®¾ç«‹|è®¾ç½®|å»ºç«‹)",
                    r"(?:æ€»éƒ¨|åŠå…¬å®¤|åˆ†å…¬å¸)(?:åœ¨|ä½äº)(.+?)",
                    r"(?:åœ°å€|åŠå…¬åœ°å€)(?:æ˜¯|ä¸º|åœ¨)(.+?)"
                ],
                "keywords": ["ä½äº", "åœ¨", "è®¾åœ¨", "åè½", "åœ°å€", "æ€»éƒ¨", "åŠå…¬å®¤", "å»ºåœ¨"],
                "distance_weight": 0.15,
                "syntax_patterns": [
                    {"pattern": r"(.+?)ä½äº(.+?)", "confidence": 0.9}
                ]
            },
            
            # äººå‘˜-æ°‘æ—å…³ç³» (æ–¹å‘: NAME -> RACE)
            "æ°‘æ—æ˜¯": {
                "subject_type": "NAME",
                "object_type": "RACE",
                "patterns": [
                    r"(?:æ°‘æ—æ˜¯|æ°‘æ—ä¸º)(.+?)æ—?",
                    r"æ˜¯(.+?)æ—äºº?",
                    r"(.+?)æ—(?:è¡€ç»Ÿ|å‡ºèº«)"
                ],
                "keywords": ["æ°‘æ—", "æ—", "è¡€ç»Ÿ", "å‡ºèº«"],
                "distance_weight": 0.1,
                "syntax_patterns": [
                    {"pattern": r"(.+?)(?:çš„)?æ°‘æ—æ˜¯(.+?)", "confidence": 0.95}
                ]
            },
            
            # äººå‘˜-ç»„ç»‡-èŒä½ä¸‰å…ƒå…³ç³»
            "åœ¨_æ‹…ä»»": {
                "subject_type": "NAME",
                "object_type": "ORG",
                "auxiliary_type": "TITLE",
                "patterns": [
                    r"åœ¨(.+?)æ‹…ä»»(.+?)",
                    r"åœ¨(.+?)(?:ä»»|åš)(.+?)",
                    r"(.+?)å…¬å¸çš„(.+?)"
                ],
                "keywords": ["åœ¨", "æ‹…ä»»", "ä»»", "åš", "çš„"],
                "is_ternary": True,
                "distance_weight": 0.3
            },
            
            # ç»„ç»‡-ç»„ç»‡å…³ç³» (æ–¹å‘: ORG -> ORG)
            "éš¶å±äº": {
                "subject_type": "ORG",
                "object_type": "ORG",
                "patterns": [
                    r"(?:éš¶å±äº|å±äº|å½’å±äº)(.+?)",
                    r"æ˜¯(.+?)(?:çš„)?(?:å­å…¬å¸|åˆ†å…¬å¸|éƒ¨é—¨)",
                    r"(.+?)(?:æ——ä¸‹|ä¸‹å±)(?:å…¬å¸|æœºæ„)"
                ],
                "keywords": ["éš¶å±", "å±äº", "å½’å±", "å­å…¬å¸", "åˆ†å…¬å¸", "éƒ¨é—¨", "æ——ä¸‹", "ä¸‹å±"],
                "distance_weight": 0.2
            }
        }
        return patterns
    
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„NERæ¨¡å‹"""
        print(f"æ­£åœ¨åŠ è½½NERæ¨¡å‹: {self.model_path}")
        
        # ç›´æ¥ä»è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„åŠ è½½é…ç½®å’Œtokenizer
        # å› ä¸ºè®­ç»ƒæ—¶å·²ç»å°†BERTé…ç½®ä¿å­˜åˆ°äº†è¾“å‡ºç›®å½•
        config = BertConfig.from_pretrained(self.model_path)
        self.tokenizer = BertTokenizer.from_pretrained(self.model_path, do_lower_case=True)
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        self.model = BertCrfForNer(config)
        
        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡
        safetensors_path = os.path.join(self.model_path, "model.safetensors")
        if os.path.exists(safetensors_path) and SAFETENSORS_AVAILABLE:
            print(f"ä» {safetensors_path} åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡...")
            state_dict = load_file(safetensors_path)
            self.model.load_state_dict(state_dict)
            print("âœ… è®­ç»ƒå¥½çš„NERæ¨¡å‹åŠ è½½æˆåŠŸï¼ˆåŒ…å«BERT+CRFï¼‰")
        else:
            print("âŒ é”™è¯¯: æœªæ‰¾åˆ° model.safetensors æˆ– safetensors åº“æœªå®‰è£…")
            return False
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        # æ£€æŸ¥è®¾å¤‡
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        
        # åŠ è½½å¤„ç†å™¨
        self.processor = CnerProcessor()
        self.label_list = self.processor.get_labels()
        self.id2label = {i: label for i, label in enumerate(self.label_list)}
        
        print(f"ğŸš€ æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {self.device}")
        print("ğŸ’¡ ä½¿ç”¨æ‚¨è®­ç»ƒå¥½çš„BERT-CRFæ¨¡å‹è¿›è¡Œå…³ç³»æŠ½å–å¢å¼ºï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œæš‚æ—¶ç¦ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼‰")
        return True
    
    def extract_entities_with_attention(self, text):
        """æå–æ–‡æœ¬ä¸­çš„å®ä½“ï¼ŒåŒæ—¶è·å–BERTéšè—çŠ¶æ€ç”¨äºå…³ç³»æŠ½å–ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å…æ³¨æ„åŠ›æƒé‡é—®é¢˜ï¼‰"""
        chars = list(text.strip())
        
        # åˆ›å»ºç¤ºä¾‹å¯¹è±¡
        example = InputExample(
            guid="relation-1",
            text_a=chars,
            labels=["O"] * len(chars)
        )
        
        # è½¬æ¢ä¸ºfeatures
        features = convert_examples_to_features(
            examples=[example],
            tokenizer=self.tokenizer,
            label_list=self.label_list,
            max_seq_length=512,
            cls_token_at_end=False,
            pad_on_left=False,
            cls_token=self.tokenizer.cls_token,
            cls_token_segment_id=0,
            sep_token=self.tokenizer.sep_token,
            pad_token=self.tokenizer.convert_tokens_to_ids([self.tokenizer.pad_token])[0],
            pad_token_segment_id=0,
        )
        
        if not features:
            return [], None, None
        
        feature = features[0]
        
        # åˆ›å»ºå¼ é‡
        input_ids = torch.tensor([feature.input_ids], dtype=torch.long).to(self.device)
        input_mask = torch.tensor([feature.input_mask], dtype=torch.long).to(self.device)
        segment_ids = torch.tensor([feature.segment_ids], dtype=torch.long).to(self.device)
        
        # é¢„æµ‹å¹¶è·å–éšè—çŠ¶æ€ï¼ˆä¸è·å–æ³¨æ„åŠ›æƒé‡ä»¥é¿å…é”™è¯¯ï¼‰
        with torch.no_grad():
            inputs = {
                "input_ids": input_ids,
                "attention_mask": input_mask,
                "labels": None
            }
            if segment_ids is not None:
                inputs["token_type_ids"] = segment_ids
            
            # è·å–BERTçš„éšè—çŠ¶æ€ï¼ˆä¸è®¾ç½®output_attentionsé¿å…é”™è¯¯ï¼‰
            try:
                # å…ˆå°è¯•è·å–éšè—çŠ¶æ€
                self.model.bert.config.output_attentions = False  # ç¡®ä¿ä¸è·å–æ³¨æ„åŠ›æƒé‡
                bert_outputs = self.model.bert(**inputs)
                hidden_states = bert_outputs.last_hidden_state  # [1, seq_len, hidden_size]
                attention_weights = None  # æš‚æ—¶ä¸ä½¿ç”¨æ³¨æ„åŠ›æƒé‡
            except Exception as e:
                print(f"è·å–BERTéšè—çŠ¶æ€å¤±è´¥: {e}")
                # å¦‚æœå¤±è´¥ï¼Œç›´æ¥è¿›è¡ŒCRFé¢„æµ‹
                hidden_states = None
                attention_weights = None
            
            # è·å–CRFé¢„æµ‹ç»“æœ
            outputs = self.model(**inputs)
            logits = outputs[0]
            tags = self.model.crf.decode(logits, inputs['attention_mask'])
            tags = tags.squeeze(0).cpu().numpy().tolist()
        
        # å¤„ç†é¢„æµ‹ç»“æœ
        preds = tags[0][1:-1]  # [CLS]XXXX[SEP] å»æ‰é¦–å°¾
        
        # æå–å®ä½“
        entities = get_entities(preds, self.id2label, 'bios')
        
        # æ„å»ºå®ä½“ä¿¡æ¯ï¼ŒåŒ…å«tokenä½ç½®æ˜ å°„
        entity_list = []
        for entity in entities:
            entity_type, start, end = entity[0], entity[1], entity[2]
            if start < len(chars) and end < len(chars):
                entity_text = "".join(chars[start:end+1])
                entity_list.append({
                    'type': entity_type,
                    'text': entity_text,
                    'start': start,
                    'end': end,
                    'token_start': start + 1,  # +1 å› ä¸ºæœ‰[CLS]
                    'token_end': end + 1,
                    'type_name': self.entity_map.get(entity_type, entity_type)
                })
        
        return entity_list, hidden_states, attention_weights
    
    def extract_entities(self, text):
        """æå–æ–‡æœ¬ä¸­çš„å®ä½“ï¼Œä½¿ç”¨BERTä¸Šä¸‹æ–‡ç†è§£"""
        entities, _, _ = self.extract_entities_with_attention(text)
        return entities
    
    def extract_relations_by_rules(self, text, entities):
        """åŸºäºè§„åˆ™å’ŒBERTä¸Šä¸‹æ–‡ç†è§£æå–å…³ç³»"""
        # è·å–BERTæ³¨æ„åŠ›å’Œéšè—çŠ¶æ€
        _, hidden_states, attention_weights = self.extract_entities_with_attention(text)
        
        relations = []
        
        # æŒ‰å®ä½“ç±»å‹åˆ†ç»„
        entities_by_type = defaultdict(list)
        for entity in entities:
            entities_by_type[entity['type']].append(entity)
        
        # å®šä¹‰å…³ç³»ä¼˜å…ˆçº§ï¼ˆé¿å…å†²çªï¼Œä¼˜å…ˆçº§é«˜çš„å…ˆå¤„ç†ï¼‰
        relation_priority = {
            "æ¯•ä¸šäº": 1,      # æœ€é«˜ä¼˜å…ˆçº§ï¼Œé¿å…è¢«"æ‹…ä»»"è¯¯åˆ¤
            "ä¸“ä¸šæ˜¯": 1,      # æ•™è‚²ç›¸å…³ï¼Œé«˜ä¼˜å…ˆçº§
            "è”ç³»æ–¹å¼": 1,    # æ˜ç¡®çš„è”ç³»ä¿¡æ¯
            "æ°‘æ—æ˜¯": 1,      # æ˜ç¡®çš„èº«ä»½ä¿¡æ¯
            "ä½äº": 2,        # åœ°ç†ä½ç½®å…³ç³»
            "å±…ä½äº": 2,      # åœ°ç†ä½ç½®å…³ç³»
            "å·¥ä½œäº": 3,      # å·¥ä½œå…³ç³»
            "æ‹…ä»»": 4,        # èŒä½å…³ç³»ï¼Œè¾ƒä½ä¼˜å…ˆçº§é¿å…è¯¯åˆ¤
            "éš¶å±äº": 5,      # ç»„ç»‡å…³ç³»
            "åœ¨_æ‹…ä»»": 6      # ä¸‰å…ƒå…³ç³»ï¼Œæœ€ä½ä¼˜å…ˆçº§
        }
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºå…³ç³»æ¨¡å¼
        sorted_relations = sorted(self.relation_patterns.items(), 
                                 key=lambda x: relation_priority.get(x[0], 999))
        
        # éå†æ‰€æœ‰å…³ç³»æ¨¡å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
        for relation_name, pattern_info in sorted_relations:
            new_relations = self._extract_binary_relations(
                text, entities_by_type, relation_name, pattern_info, attention_weights, hidden_states
            )
            
            # å†²çªæ£€æµ‹ï¼šé¿å…åŒä¸€å¯¹å®ä½“è¢«åˆ†é…å¤šä¸ªå†²çªçš„å…³ç³»
            filtered_relations = self._filter_conflicting_relations(new_relations, relations)
            relations.extend(filtered_relations)
            
            # å¤„ç†ä¸‰å…ƒå…³ç³»
            if pattern_info.get('is_ternary', False):
                ternary_relations = self._extract_ternary_relations(
                    text, entities_by_type, relation_name, pattern_info, attention_weights, hidden_states
                )
                relations.extend(ternary_relations)
        
        # å»é‡å’Œæ’åº
        relations = self._deduplicate_relations(relations)
        relations.sort(key=lambda x: x['confidence'], reverse=True)
        
        return relations
    
    def _extract_binary_relations(self, text, entities_by_type, relation_name, pattern_info, attention_weights=None, hidden_states=None):
        """æå–äºŒå…ƒå…³ç³»ï¼Œé›†æˆBERTä¸Šä¸‹æ–‡ç†è§£"""
        relations = []
        subject_type = pattern_info['subject_type']
        object_type = pattern_info['object_type']
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¿…éœ€çš„å®ä½“ç±»å‹
        if not entities_by_type.get(subject_type) or not entities_by_type.get(object_type):
            return relations
        
        # åŸºäºå…³é”®è¯çš„ç®€å•åŒ¹é…ï¼Œä½†è¦æ›´ç²¾ç¡®åœ°æ£€æŸ¥ä¸Šä¸‹æ–‡
        keywords = pattern_info.get('keywords', [])
        has_keywords = any(keyword in text for keyword in keywords)
        if not has_keywords:
            return relations
        
        # ç‰¹æ®Šå¤„ç†ï¼šç²¾ç¡®åŒ¹é…å…³é”®æ¨¡å¼ä»¥é¿å…è¯¯åˆ¤
        if relation_name == "æ¯•ä¸šäº":
            # ç¡®ä¿çœŸçš„æ˜¯æ¯•ä¸šå…³ç³»ï¼Œè€Œä¸æ˜¯å·¥ä½œå…³ç³»
            graduation_patterns = ["æ¯•ä¸šäº", "æ¯•ä¸šè‡ª", "å°±è¯»äº", "åœ¨è¯»äº"]
            if not any(pattern in text for pattern in graduation_patterns):
                return relations
        elif relation_name == "æ‹…ä»»":
            # ç¡®ä¿ä¸æ˜¯æ¯•ä¸šå…³ç³»è¢«è¯¯åˆ¤ä¸ºæ‹…ä»»
            if any(pattern in text for pattern in ["æ¯•ä¸šäº", "æ¯•ä¸šè‡ª", "å°±è¯»äº", "åœ¨è¯»äº"]):
                # å¦‚æœå­˜åœ¨æ¯•ä¸šå…³é”®è¯ï¼Œé™ä½æ‹…ä»»å…³ç³»çš„æ£€æµ‹æ•æ„Ÿåº¦
                pass
        
        # æå–å…³ç³» - ç¡®ä¿æ–¹å‘æ­£ç¡®
        for subject_entity in entities_by_type[subject_type]:
            for object_entity in entities_by_type[object_type]:
                # æ£€æŸ¥å®ä½“é—´çš„è·ç¦»å’Œä½ç½®å…³ç³»
                distance = abs(subject_entity['start'] - object_entity['start'])
                max_distance = 30  # å¢åŠ æœ€å¤§è·ç¦»
                
                if distance < max_distance:
                    # åŸºäºä½ç½®ã€è¯­æ³•å’ŒBERTä¸Šä¸‹æ–‡æ£€æŸ¥å…³ç³»æ–¹å‘çš„åˆç†æ€§
                    confidence = self._calculate_enhanced_confidence(
                        text, subject_entity, object_entity, pattern_info, relation_name, attention_weights, hidden_states
                    )
                    
                    if confidence > 0.25:  # é™ä½é˜ˆå€¼ä»¥è·å–æ›´å¤šå…³ç³»
                        relation = {
                            'relation': relation_name,
                            'subject': subject_entity,
                            'object': object_entity,
                            'confidence': confidence,
                            'context': self._extract_context(text, subject_entity, object_entity),
                            'direction': f"{subject_type} -> {object_type}",
                            'type': 'binary'
                        }
                        relations.append(relation)
        
        return relations
    
    def _extract_ternary_relations(self, text, entities_by_type, relation_name, pattern_info, attention_weights=None, hidden_states=None):
        """æå–ä¸‰å…ƒå…³ç³»ï¼ˆäººå‘˜-ç»„ç»‡-èŒä½ï¼‰ï¼Œé›†æˆBERTä¸Šä¸‹æ–‡ç†è§£"""
        relations = []
        subject_type = pattern_info['subject_type']
        object_type = pattern_info['object_type']
        auxiliary_type = pattern_info.get('auxiliary_type')
        
        if not all([entities_by_type.get(subject_type), entities_by_type.get(object_type), 
                   entities_by_type.get(auxiliary_type)]):
            return relations
        
        keywords = pattern_info.get('keywords', [])
        has_keywords = any(keyword in text for keyword in keywords)
        if not has_keywords:
            return relations
        
        # ä¸‰å…ƒå…³ç³»æå–
        for name_entity in entities_by_type[subject_type]:
            for org_entity in entities_by_type[object_type]:
                for title_entity in entities_by_type[auxiliary_type]:
                    # è®¡ç®—ä¸‰ä¸ªå®ä½“é—´çš„æ€»è·ç¦»
                    entities_positions = [name_entity['start'], org_entity['start'], title_entity['start']]
                    total_span = max(entities_positions) - min(entities_positions)
                    
                    if total_span < 50:  # ä¸‰å…ƒå…³ç³»å…è®¸æ›´å¤§è·ç¦»
                        confidence = self._calculate_ternary_confidence(
                            text, name_entity, org_entity, title_entity, pattern_info, attention_weights, hidden_states
                        )
                        
                        if confidence > 0.4:
                            relation = {
                                'relation': relation_name,
                                'subject': name_entity,
                                'object': org_entity,
                                'auxiliary': title_entity,
                                'confidence': confidence,
                                'context': self._extract_extended_context(text, [name_entity, org_entity, title_entity]),
                                'direction': f"{subject_type} -> {object_type} ({auxiliary_type})",
                                'type': 'ternary'
                            }
                            relations.append(relation)
        
        return relations
    
    def _calculate_enhanced_confidence(self, text, subject_entity, object_entity, pattern_info, relation_name, attention_weights=None, hidden_states=None):
        """å¢å¼ºçš„ç½®ä¿¡åº¦è®¡ç®—ç®—æ³•ï¼Œé›†æˆBERTæ³¨æ„åŠ›æœºåˆ¶"""
        confidence = 0.2  # åŸºç¡€ç½®ä¿¡åº¦
        
        subject_start = subject_entity['start']
        object_start = object_entity['start']
        subject_text = subject_entity['text']
        object_text = object_entity['text']
        
        # 1. æ£€æŸ¥å…³é”®è¯å¯†åº¦
        keywords = pattern_info.get('keywords', [])
        keyword_count = sum(1 for keyword in keywords if keyword in text)
        confidence += keyword_count * 0.12
        
        # 2. æ£€æŸ¥è¯­æ³•æ¨¡å¼åŒ¹é…
        syntax_patterns = pattern_info.get('syntax_patterns', [])
        for syntax_pattern in syntax_patterns:
            if re.search(syntax_pattern['pattern'], text):
                confidence += syntax_pattern.get('confidence', 0.2)
                break
        
        # 3. å®ä½“é—´è·ç¦»åŠ æƒ
        distance = abs(subject_start - object_start)
        distance_weight = pattern_info.get('distance_weight', 0.15)
        if distance < 5:
            confidence += distance_weight * 2
        elif distance < 10:
            confidence += distance_weight * 1.5
        elif distance < 15:
            confidence += distance_weight
        elif distance < 25:
            confidence += distance_weight * 0.5
        
        # 4. æ¨¡å¼åŒ¹é…åŠ åˆ†
        patterns = pattern_info.get('patterns', [])
        for pattern in patterns:
            if re.search(pattern, text):
                confidence += 0.25
                break
        
        # 5. BERTæ³¨æ„åŠ›æƒé‡åˆ†æï¼ˆæ–°å¢ï¼‰
        if attention_weights is not None and hidden_states is not None:
            attention_confidence = self._analyze_attention_patterns(
                subject_entity, object_entity, attention_weights, hidden_states
            )
            confidence += attention_confidence
        
        # 6. è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æï¼ˆæ–°å¢ï¼‰
        if hidden_states is not None:
            semantic_confidence = self._calculate_semantic_similarity(
                subject_entity, object_entity, hidden_states, relation_name
            )
            confidence += semantic_confidence
        
        # 7. æ–¹å‘æ€§æ£€æŸ¥
        directional_confidence = self._check_directional_logic(
            text, subject_entity, object_entity, relation_name
        )
        confidence += directional_confidence
        
        # 8. ä¸Šä¸‹æ–‡è¯­ä¹‰ä¸€è‡´æ€§
        context_confidence = self._check_context_consistency(
            text, subject_entity, object_entity, relation_name
        )
        confidence += context_confidence
        
        # 9. å®ä½“ç±»å‹åŒ¹é…åº¦
        type_confidence = self._check_entity_type_compatibility(
            subject_entity, object_entity, relation_name
        )
        confidence += type_confidence
        
        return max(0.0, min(confidence, 1.0))
    
    def _analyze_attention_patterns(self, subject_entity, object_entity, attention_weights, hidden_states):
        """åˆ†æBERTæ³¨æ„åŠ›æ¨¡å¼ä»¥å¢å¼ºå…³ç³»æŠ½å–ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œæš‚æ—¶ç¦ç”¨ï¼‰"""
        # æš‚æ—¶ç¦ç”¨æ³¨æ„åŠ›åˆ†æä»¥é¿å…é”™è¯¯ï¼Œè¿”å›å›ºå®šçš„ç½®ä¿¡åº¦å¢é‡
        if attention_weights is None:
            # åŸºäºå®ä½“ä½ç½®çš„ç®€å•è·ç¦»è®¡ç®—
            distance = abs(subject_entity['start'] - object_entity['start'])
            if distance < 5:
                return 0.15  # è·ç¦»å¾ˆè¿‘ï¼Œç»™äºˆè¾ƒé«˜çš„ç½®ä¿¡åº¦å¢é‡
            elif distance < 10:
                return 0.1   # è·ç¦»ä¸­ç­‰ï¼Œç»™äºˆä¸­ç­‰çš„ç½®ä¿¡åº¦å¢é‡
            elif distance < 20:
                return 0.05  # è·ç¦»è¾ƒè¿œï¼Œç»™äºˆè¾ƒä½çš„ç½®ä¿¡åº¦å¢é‡
            else:
                return 0.0   # è·ç¦»å¤ªè¿œï¼Œä¸ç»™ç½®ä¿¡åº¦å¢é‡
        
        # å¦‚æœæœ‰æ³¨æ„åŠ›æƒé‡æ•°æ®ï¼Œå°è¯•åˆ†æï¼ˆä½†ç”±äºå…¼å®¹æ€§é—®é¢˜ï¼Œè¿™éƒ¨åˆ†æš‚æ—¶è¢«ç¦ç”¨ï¼‰
        try:
            # ç”±äºBERTæ³¨æ„åŠ›æœºåˆ¶çš„å…¼å®¹æ€§é—®é¢˜ï¼Œæš‚æ—¶è¿”å›åŸºäºè·ç¦»çš„ç®€å•è®¡ç®—
            distance = abs(subject_entity['start'] - object_entity['start'])
            return max(0.0, 0.2 - distance * 0.01)  # è·ç¦»è¶Šè¿‘ï¼Œç½®ä¿¡åº¦å¢é‡è¶Šé«˜
            
        except Exception as e:
            print(f"æ³¨æ„åŠ›åˆ†æå¤±è´¥ï¼ˆå·²è·³è¿‡ï¼‰: {e}")
            return 0.0
    
    def _calculate_semantic_similarity(self, subject_entity, object_entity, hidden_states, relation_name):
        """è®¡ç®—åŸºäºBERTéšè—çŠ¶æ€çš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
        try:
            # å¦‚æœéšè—çŠ¶æ€ä¸ºç©ºï¼Œè¿”å›0
            if hidden_states is None:
                return 0.0
                
            # è·å–å®ä½“çš„éšè—çŠ¶æ€è¡¨ç¤º
            subj_start = subject_entity.get('token_start', subject_entity['start'] + 1)
            subj_end = subject_entity.get('token_end', subject_entity['end'] + 1)
            obj_start = object_entity.get('token_start', object_entity['start'] + 1)
            obj_end = object_entity.get('token_end', object_entity['end'] + 1)
            
            # æå–å®ä½“çš„å¹³å‡éšè—çŠ¶æ€
            hidden_states_cpu = hidden_states.cpu()
            
            # è¾¹ç•Œæ£€æŸ¥
            max_seq_len = hidden_states_cpu.shape[1]
            subj_start = min(subj_start, max_seq_len - 1)
            subj_end = min(subj_end, max_seq_len - 1)
            obj_start = min(obj_start, max_seq_len - 1)
            obj_end = min(obj_end, max_seq_len - 1)
            
            if subj_start > subj_end or obj_start > obj_end:
                return 0.0
            
            subj_hidden = hidden_states_cpu[0, subj_start:subj_end+1].mean(dim=0)  # [hidden_size]
            obj_hidden = hidden_states_cpu[0, obj_start:obj_end+1].mean(dim=0)  # [hidden_size]
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = F.cosine_similarity(subj_hidden.unsqueeze(0), obj_hidden.unsqueeze(0), dim=1).item()
            
            # æ ¹æ®å…³ç³»ç±»å‹è°ƒæ•´ç›¸ä¼¼åº¦æƒé‡
            relation_weights = {
                "å·¥ä½œäº": 0.15,      # äººåå’Œç»„ç»‡åº”è¯¥æœ‰ä¸€å®šç›¸å…³æ€§
                "æ‹…ä»»": 0.2,        # äººåå’ŒèŒä½ç›¸å…³æ€§è¾ƒé«˜
                "å±…ä½äº": 0.1,      # äººåå’Œåœ°å€ç›¸å…³æ€§è¾ƒä½
                "æ¯•ä¸šäº": 0.15,     # äººåå’Œå­¦æ ¡æœ‰ä¸€å®šç›¸å…³æ€§
                "ä¸“ä¸šæ˜¯": 0.2,      # äººåå’Œä¸“ä¸šç›¸å…³æ€§è¾ƒé«˜
                "ä½äº": 0.1,        # ç»„ç»‡å’Œåœ°å€ç›¸å…³æ€§è¾ƒä½
                "è”ç³»æ–¹å¼": 0.05,   # äººåå’Œè”ç³»æ–¹å¼ç›¸å…³æ€§æœ€ä½
                "æ°‘æ—æ˜¯": 0.1,      # äººåå’Œæ°‘æ—ç›¸å…³æ€§è¾ƒä½
                "éš¶å±äº": 0.2       # ç»„ç»‡é—´ç›¸å…³æ€§è¾ƒé«˜
            }
            
            weight = relation_weights.get(relation_name, 0.1)
            semantic_confidence = similarity * weight
            
            return max(0.0, min(semantic_confidence, 0.15))  # æœ€å¤§è´¡çŒ®0.15
            
        except Exception as e:
            print(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _check_directional_logic(self, text, subject_entity, object_entity, relation_name):
        """æ£€æŸ¥å…³ç³»æ–¹å‘çš„é€»è¾‘åˆç†æ€§"""
        subject_start = subject_entity['start']
        object_start = object_entity['start']
        subject_text = subject_entity['text']
        object_text = object_entity['text']
        
        # æå–å®ä½“é—´çš„ä¸Šä¸‹æ–‡
        start_pos = min(subject_start, object_start)
        end_pos = max(subject_entity['end'], object_entity['end'])
        context = text[max(0, start_pos-5):end_pos + 6]
        
        directional_bonus = 0.0
        
        if relation_name == "æ‹…ä»»":
            # "å¼ ä¸‰æ‹…ä»»äº§å“ç»ç†" vs "äº§å“ç»ç†æ‹…ä»»å¼ ä¸‰"
            if any(keyword in context for keyword in ["æ‹…ä»»", "å‡ºä»»", "æ˜¯"]):
                if subject_start < object_start:
                    directional_bonus += 0.3
                else:
                    directional_bonus -= 0.15
        
        elif relation_name == "å·¥ä½œäº":
            # "å¼ ä¸‰åœ¨é˜¿é‡Œå·¥ä½œ" vs "é˜¿é‡Œåœ¨å¼ ä¸‰å·¥ä½œ"
            if "åœ¨" in context and "å·¥ä½œ" in context:
                zai_pos = context.find("åœ¨")
                work_pos = context.find("å·¥ä½œ")
                if zai_pos < work_pos:
                    if subject_start < object_start:
                        directional_bonus += 0.35
                    else:
                        directional_bonus -= 0.25
        
        elif relation_name == "å±…ä½äº":
            if any(keyword in context for keyword in ["ä½åœ¨", "å±…ä½åœ¨", "å®¶åœ¨"]):
                if subject_start < object_start:
                    directional_bonus += 0.3
                else:
                    directional_bonus -= 0.4
        
        elif relation_name == "æ¯•ä¸šäº":
            if "æ¯•ä¸šäº" in context:
                if subject_start < object_start:
                    directional_bonus += 0.35
                else:
                    directional_bonus -= 0.45
        
        elif relation_name == "ä½äº":
            if "ä½äº" in context:
                if subject_start < object_start:
                    directional_bonus += 0.3
                else:
                    directional_bonus -= 0.3
        
        elif relation_name == "ä¸“ä¸šæ˜¯":
            if "ä¸“ä¸š" in context:
                if subject_start < object_start:
                    directional_bonus += 0.25
                else:
                    directional_bonus -= 0.2
        
        return directional_bonus
    
    def _check_context_consistency(self, text, subject_entity, object_entity, relation_name):
        """æ£€æŸ¥ä¸Šä¸‹æ–‡è¯­ä¹‰ä¸€è‡´æ€§"""
        context = self._extract_context(text, subject_entity, object_entity)
        context_bonus = 0.0
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨çŸ›ç›¾çš„è¡¨è¿°
        contradiction_patterns = [
            r"ä¸åœ¨(.+?)å·¥ä½œ",
            r"ä¸æ˜¯(.+?)",
            r"æ²¡æœ‰(.+?)ç»éªŒ",
            r"ä»æ²¡æœ‰(.+?)"
        ]
        
        for pattern in contradiction_patterns:
            if re.search(pattern, context):
                context_bonus -= 0.3
                break
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼ºåŒ–è¡¨è¿°
        enhancement_patterns = [
            r"ä¸€ç›´åœ¨(.+?)",
            r"ä¸€ç›´æ˜¯(.+?)",
            r"ä¸“é—¨(.+?)",
            r"ä¸»è¦(.+?)",
            r"å½“å‰(.+?)"
        ]
        
        for pattern in enhancement_patterns:
            if re.search(pattern, context):
                context_bonus += 0.15
                break
        
        return context_bonus
    
    def _check_entity_type_compatibility(self, subject_entity, object_entity, relation_name):
        """æ£€æŸ¥å®ä½“ç±»å‹å…¼å®¹æ€§"""
        subject_type = subject_entity['type']
        object_type = object_entity['type']
        
        # å®šä¹‰åˆç†çš„å®ä½“ç±»å‹ç»„åˆ
        valid_combinations = {
            "æ‹…ä»»": [("NAME", "TITLE")],
            "å·¥ä½œäº": [("NAME", "ORG")],
            "å±…ä½äº": [("NAME", "LOC")],
            "æ¯•ä¸šäº": [("NAME", "EDU")],
            "ä¸“ä¸šæ˜¯": [("NAME", "PRO")],
            "ä½äº": [("ORG", "LOC")],
            "è”ç³»æ–¹å¼": [("NAME", "CONT")],
            "æ°‘æ—æ˜¯": [("NAME", "RACE")],
            "éš¶å±äº": [("ORG", "ORG")]
        }
        
        if relation_name in valid_combinations:
            if (subject_type, object_type) in valid_combinations[relation_name]:
                return 0.2
            else:
                return -0.3
        
        return 0.0
    
    def _calculate_ternary_confidence(self, text, name_entity, org_entity, title_entity, pattern_info, attention_weights=None, hidden_states=None):
        """è®¡ç®—ä¸‰å…ƒå…³ç³»çš„ç½®ä¿¡åº¦ï¼Œé›†æˆBERTä¸Šä¸‹æ–‡ç†è§£"""
        confidence = 0.3
        
        # æ£€æŸ¥å…³é”®è¯
        keywords = pattern_info.get('keywords', [])
        keyword_count = sum(1 for keyword in keywords if keyword in text)
        confidence += keyword_count * 0.1
        
        # æ£€æŸ¥ä¸‰ä¸ªå®ä½“çš„ç›¸å¯¹ä½ç½®
        entities_positions = [
            (name_entity['start'], 'NAME'),
            (org_entity['start'], 'ORG'),
            (title_entity['start'], 'TITLE')
        ]
        entities_positions.sort()
        
        # æ£€æŸ¥æ˜¯å¦ç¬¦åˆå¸¸è§çš„è¯­è¨€æ¨¡å¼
        # å¸¸è§æ¨¡å¼: äººå + åœ¨ + ç»„ç»‡ + æ‹…ä»» + èŒä½
        pos_pattern = [pos[1] for pos in entities_positions]
        if pos_pattern == ['NAME', 'ORG', 'TITLE'] or pos_pattern == ['NAME', 'TITLE', 'ORG']:
            confidence += 0.2
        
        # æ£€æŸ¥æ¨¡å¼åŒ¹é…
        patterns = pattern_info.get('patterns', [])
        for pattern in patterns:
            if re.search(pattern, text):
                confidence += 0.2
                break
        
        # BERTå¢å¼ºï¼šåˆ†æä¸‰å…ƒå…³ç³»çš„æ³¨æ„åŠ›æ¨¡å¼
        if attention_weights is not None and hidden_states is not None:
            try:
                # åˆ†æä¸‰ä¸ªå®ä½“é—´çš„æ³¨æ„åŠ›å…³è”
                entities = [name_entity, org_entity, title_entity]
                attention_score = self._analyze_ternary_attention(entities, attention_weights)
                confidence += attention_score * 0.15
                
                # åˆ†æè¯­ä¹‰ä¸€è‡´æ€§
                semantic_score = self._analyze_ternary_semantics(entities, hidden_states)
                confidence += semantic_score * 0.1
                
            except Exception as e:
                print(f"ä¸‰å…ƒå…³ç³»BERTåˆ†æå¤±è´¥: {e}")
        
        return min(confidence, 1.0)
    
    def _analyze_ternary_attention(self, entities, attention_weights):
        """åˆ†æä¸‰å…ƒå…³ç³»çš„æ³¨æ„åŠ›æ¨¡å¼"""
        try:
            total_attention = 0.0
            
            # è®¡ç®—ä¸‰ä¸ªå®ä½“é—´çš„ç›¸äº’æ³¨æ„åŠ›
            for i, entity1 in enumerate(entities):
                for j, entity2 in enumerate(entities):
                    if i != j:
                        start1 = entity1.get('token_start', entity1['start'] + 1)
                        end1 = entity1.get('token_end', entity1['end'] + 1)
                        start2 = entity2.get('token_start', entity2['start'] + 1)
                        end2 = entity2.get('token_end', entity2['end'] + 1)
                        
                        # åˆ†ææœ€åä¸¤å±‚çš„æ³¨æ„åŠ›
                        for layer_idx in range(10, 12):
                            if layer_idx < len(attention_weights):
                                layer_attention = attention_weights[layer_idx].squeeze(0)
                                
                                for head in range(layer_attention.shape[0]):
                                    for pos1 in range(start1, min(end1 + 1, layer_attention.shape[1])):
                                        for pos2 in range(start2, min(end2 + 1, layer_attention.shape[2])):
                                            total_attention += layer_attention[head, pos1, pos2].item()
            
            # å½’ä¸€åŒ–
            num_pairs = len(entities) * (len(entities) - 1)
            if num_pairs > 0:
                return min(total_attention / (num_pairs * 24), 0.3)  # 12å¤´ * 2å±‚ = 24
            
            return 0.0
            
        except Exception:
            return 0.0
    
    def _analyze_ternary_semantics(self, entities, hidden_states):
        """åˆ†æä¸‰å…ƒå…³ç³»çš„è¯­ä¹‰ä¸€è‡´æ€§"""
        try:
            # è·å–ä¸‰ä¸ªå®ä½“çš„éšè—çŠ¶æ€è¡¨ç¤º
            entity_representations = []
            
            for entity in entities:
                start = entity.get('token_start', entity['start'] + 1)
                end = entity.get('token_end', entity['end'] + 1)
                entity_hidden = hidden_states[0, start:end+1].mean(dim=0)
                entity_representations.append(entity_hidden)
            
            # è®¡ç®—ä¸‰ä¸ªå®ä½“é—´çš„å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦
            similarities = []
            for i in range(len(entity_representations)):
                for j in range(i + 1, len(entity_representations)):
                    sim = F.cosine_similarity(
                        entity_representations[i].unsqueeze(0),
                        entity_representations[j].unsqueeze(0),
                        dim=1
                    ).item()
                    similarities.append(sim)
            
            # è¿”å›å¹³å‡ç›¸ä¼¼åº¦ï¼Œä½†æƒé‡è¾ƒä½ï¼ˆä¸‰å…ƒå…³ç³»ä¸­å®ä½“ç±»å‹å·®å¼‚è¾ƒå¤§ï¼‰
            if similarities:
                return np.mean(similarities) * 0.1
            
            return 0.0
            
        except Exception:
            return 0.0
    
    def _extract_extended_context(self, text, entities):
        """æå–æ‰©å±•ä¸Šä¸‹æ–‡ï¼ˆç”¨äºå¤šå®ä½“ï¼‰"""
        positions = [entity['start'] for entity in entities] + [entity['end'] for entity in entities]
        start = min(positions)
        end = max(positions)
        
        # æ‰©å±•ä¸Šä¸‹æ–‡
        context_start = max(0, start - 15)
        context_end = min(len(text), end + 15)
        
        return text[context_start:context_end + 1]
    
    def cross_sentence_relation_extraction(self, text_list):
        """è·¨å¥å­å…³ç³»æŠ½å–"""
        all_entities = []
        all_relations = []
        sentence_boundaries = []
        
        # ä¸ºæ¯ä¸ªå¥å­æå–å®ä½“
        current_offset = 0
        for i, sentence in enumerate(text_list):
            entities = self.extract_entities(sentence)
            
            # è°ƒæ•´å®ä½“ä½ç½®ä»¥é€‚åº”å…¨æ–‡åç§»
            for entity in entities:
                entity['global_start'] = entity['start'] + current_offset
                entity['global_end'] = entity['end'] + current_offset
                entity['sentence_id'] = i
                all_entities.append(entity)
            
            sentence_boundaries.append((current_offset, current_offset + len(sentence)))
            current_offset += len(sentence) + 1  # +1 ä¸ºå¥å­é—´çš„åˆ†éš”ç¬¦
        
        # å…ˆå¤„ç†å¥å†…å…³ç³»
        for i, sentence in enumerate(text_list):
            sentence_entities = [e for e in all_entities if e['sentence_id'] == i]
            sentence_relations = self.extract_relations_by_rules(sentence, sentence_entities)
            for relation in sentence_relations:
                relation['scope'] = 'intra-sentence'
                relation['sentence_id'] = i
            all_relations.extend(sentence_relations)
        
        # å¤„ç†è·¨å¥å­å…³ç³»
        full_text = ' '.join(text_list)
        cross_relations = self._extract_cross_sentence_relations(all_entities, full_text, sentence_boundaries)
        all_relations.extend(cross_relations)
        
        return {
            'text_list': text_list,
            'full_text': full_text,
            'entities': all_entities,
            'relations': all_relations,
            'sentence_boundaries': sentence_boundaries
        }
    
    def _filter_conflicting_relations(self, new_relations, existing_relations):
        """è¿‡æ»¤å†²çªçš„å…³ç³»ï¼Œä¿ç•™ä¼˜å…ˆçº§æ›´é«˜çš„å…³ç³»"""
        filtered = []
        
        for new_rel in new_relations:
            has_conflict = False
            
            for existing_rel in existing_relations:
                # æ£€æŸ¥æ˜¯å¦ä¸ºåŒä¸€å¯¹å®ä½“
                if (new_rel['subject']['text'] == existing_rel['subject']['text'] and 
                    new_rel['object']['text'] == existing_rel['object']['text']):
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºå†²çªçš„å…³ç³»ç±»å‹
                    conflicting_pairs = [
                        ("æ¯•ä¸šäº", "æ‹…ä»»"),
                        ("æ¯•ä¸šäº", "å·¥ä½œäº"),
                        ("ä¸“ä¸šæ˜¯", "æ‹…ä»»"),
                        ("è”ç³»æ–¹å¼", "æ‹…ä»»")
                    ]
                    
                    for pair in conflicting_pairs:
                        if ((new_rel['relation'] == pair[0] and existing_rel['relation'] == pair[1]) or
                            (new_rel['relation'] == pair[1] and existing_rel['relation'] == pair[0])):
                            has_conflict = True
                            break
                    
                    if has_conflict:
                        break
            
            if not has_conflict:
                filtered.append(new_rel)
        
        return filtered
    
    def _extract_cross_sentence_relations(self, entities, full_text, sentence_boundaries):
        """æå–è·¨å¥å­å…³ç³»"""
        cross_relations = []
        
        # æŒ‰å®ä½“ç±»å‹åˆ†ç»„
        entities_by_type = defaultdict(list)
        for entity in entities:
            entities_by_type[entity['type']].append(entity)
        
        # åªå¤„ç†ç‰¹å®šç±»å‹çš„è·¨å¥å­å…³ç³»
        cross_sentence_patterns = {
            "å·¥ä½œäº": {
                "subject_type": "NAME", "object_type": "ORG",
                "max_distance": 2, "keywords": ["å·¥ä½œ", "å…¬å¸", "ç»„ç»‡"]
            },
            "æ‹…ä»»": {
                "subject_type": "NAME", "object_type": "TITLE",
                "max_distance": 2, "keywords": ["æ‹…ä»»", "èŒä½", "å²—ä½"]
            },
            "å±…ä½äº": {
                "subject_type": "NAME", "object_type": "LOC",
                "max_distance": 3, "keywords": ["ä½", "å®¶", "åœ°å€"]
            }
        }
        
        for relation_name, pattern_info in cross_sentence_patterns.items():
            subject_type = pattern_info['subject_type']
            object_type = pattern_info['object_type']
            max_distance = pattern_info['max_distance']
            keywords = pattern_info['keywords']
            
            if not entities_by_type.get(subject_type) or not entities_by_type.get(object_type):
                continue
            
            for subject_entity in entities_by_type[subject_type]:
                for object_entity in entities_by_type[object_type]:
                    # æ£€æŸ¥æ˜¯å¦åœ¨ä¸åŒå¥å­ä¸­
                    if subject_entity['sentence_id'] == object_entity['sentence_id']:
                        continue
                    
                    # æ£€æŸ¥å¥å­è·ç¦»
                    sentence_distance = abs(subject_entity['sentence_id'] - object_entity['sentence_id'])
                    if sentence_distance > max_distance:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³å…³é”®è¯
                    context_start = min(subject_entity['global_start'], object_entity['global_start'])
                    context_end = max(subject_entity['global_end'], object_entity['global_end'])
                    context = full_text[max(0, context_start-20):context_end+20]
                    
                    has_keywords = any(keyword in context for keyword in keywords)
                    if not has_keywords:
                        continue
                    
                    # è®¡ç®—è·¨å¥å­å…³ç³»çš„ç½®ä¿¡åº¦
                    confidence = self._calculate_cross_sentence_confidence(
                        subject_entity, object_entity, context, pattern_info, sentence_distance
                    )
                    
                    if confidence > 0.3:
                        relation = {
                            'relation': relation_name,
                            'subject': subject_entity,
                            'object': object_entity,
                            'confidence': confidence,
                            'context': context[:50] + "..." if len(context) > 50 else context,
                            'direction': f"{subject_type} -> {object_type}",
                            'type': 'cross-sentence',
                            'scope': 'inter-sentence',
                            'sentence_distance': sentence_distance
                        }
                        cross_relations.append(relation)
        
        return cross_relations
    
    def _calculate_cross_sentence_confidence(self, subject_entity, object_entity, context, pattern_info, sentence_distance):
        """è®¡ç®—è·¨å¥å­å…³ç³»çš„ç½®ä¿¡åº¦"""
        confidence = 0.2  # è·¨å¥å­å…³ç³»çš„åŸºç¡€ç½®ä¿¡åº¦è¾ƒä½
        
        # æ ¹æ®å¥å­è·ç¦»è°ƒæ•´
        if sentence_distance == 1:
            confidence += 0.2  # ç›¸é‚»å¥å­
        elif sentence_distance == 2:
            confidence += 0.1
        else:
            confidence += 0.05
        
        # æ£€æŸ¥å…³é”®è¯å¯†åº¦
        keywords = pattern_info.get('keywords', [])
        keyword_count = sum(1 for keyword in keywords if keyword in context)
        confidence += keyword_count * 0.1
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼ºè¿æ¥è¯
        strong_connectors = ["å¦å¤–", "åŒæ—¶", "æ­¤å¤–", "è¿˜æœ‰", "å¹¶ä¸”", "è€Œä¸”"]
        if any(connector in context for connector in strong_connectors):
            confidence += 0.15
        
        return confidence
    
    def _extract_context(self, text, entity1, entity2):
        """æå–å®ä½“é—´çš„ä¸Šä¸‹æ–‡"""
        start = min(entity1['start'], entity2['start'])
        end = max(entity1['end'], entity2['end'])
        
        # æ‰©å±•ä¸Šä¸‹æ–‡
        context_start = max(0, start - 10)
        context_end = min(len(text), end + 10)
        
        return text[context_start:context_end + 1]
    
    def _deduplicate_relations(self, relations):
        """å…³ç³»å»é‡ - è€ƒè™‘æ–¹å‘æ€§"""
        seen = set()
        unique_relations = []
        
        for relation in relations:
            # ä½¿ç”¨ä¸»ä½“å’Œå®¢ä½“çš„ç¡®å®šé¡ºåºä½œä¸ºå»é‡é”®
            key = (
                relation['relation'],
                relation['subject']['text'],
                relation['object']['text'],
                relation['direction']  # åŒ…å«æ–¹å‘ä¿¡æ¯
            )
            if key not in seen:
                seen.add(key)
                unique_relations.append(relation)
        
        return unique_relations
    
    def extract_relations(self, text):
        """ä¸»å‡½æ•°ï¼šä½¿ç”¨BERTå¢å¼ºæ¨¡å¼æå–æ–‡æœ¬ä¸­çš„å®ä½“å’Œå…³ç³»"""
        print(f"\næ­£åœ¨ä½¿ç”¨BERTå¢å¼ºæ¨¡å¼åˆ†ææ–‡æœ¬ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰: {text}")
        
        # 1. æå–å®ä½“å¹¶è·å–BERTéšè—çŠ¶æ€ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        entities, hidden_states, attention_weights = self.extract_entities_with_attention(text)
        print(f"è¯†åˆ«åˆ° {len(entities)} ä¸ªå®ä½“ï¼ˆå«BERTä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰")
        
        # 2. åŸºäºè§„åˆ™å’ŒBERTä¸Šä¸‹æ–‡æå–å…³ç³»ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        relations = self.extract_relations_by_rules(text, entities)
        print(f"æŠ½å–åˆ° {len(relations)} ä¸ªå…³ç³»ï¼ˆBERTå¢å¼ºç®€åŒ–ç‰ˆï¼‰")
        
        return {
            'text': text,
            'entities': entities,
            'relations': relations
        }
    
    def format_results(self, result):
        """æ ¼å¼åŒ–æ˜¾ç¤ºç»“æœ"""
        text = result.get('text', result.get('full_text', ''))
        entities = result['entities']
        relations = result['relations']
        
        print("\n" + "="*80)
        print("å®ä½“è¯†åˆ«ä¸å…³ç³»æŠ½å–ç»“æœ")
        print("="*80)
        
        if result.get('text_list'):
            print(f"\nè¾“å…¥æ–‡æœ¬ï¼ˆå¤šå¥ï¼‰:")
            for i, sentence in enumerate(result['text_list']):
                print(f"  {i+1}. {sentence}")
        else:
            print(f"\nè¾“å…¥æ–‡æœ¬: {text}")
        
        # æ˜¾ç¤ºå®ä½“
        print(f"\nè¯†åˆ«çš„å®ä½“ ({len(entities)}ä¸ª):")
        if entities:
            print("-" * 80)
            print(f"{'åºå·':<4} {'å®ä½“ç±»å‹':<12} {'å®ä½“æ–‡æœ¬':<20} {'ä½ç½®':<15} {'å¥å­ID':<8}")
            print("-" * 80)
            for i, entity in enumerate(entities, 1):
                sentence_info = f"å¥{entity.get('sentence_id', 'N/A')}" if 'sentence_id' in entity else 'N/A'
                position = f"{entity['start']}-{entity['end']}"
                if 'global_start' in entity:
                    position += f" (å…¨å±€{entity['global_start']}-{entity['global_end']})"
                print(f"{i:<4} {entity['type_name']:<12} {entity['text']:<20} {position:<15} {sentence_info:<8}")
        else:
            print("  æœªè¯†åˆ«åˆ°å®ä½“")
        
        # æ˜¾ç¤ºå…³ç³»
        print(f"\næŠ½å–çš„å…³ç³» ({len(relations)}ä¸ª):")
        if relations:
            # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤º
            intra_relations = [r for r in relations if r.get('scope') == 'intra-sentence' or r.get('type') in ['binary', 'ternary']]
            inter_relations = [r for r in relations if r.get('scope') == 'inter-sentence']
            
            if intra_relations:
                print("\nå¥å†…å…³ç³»:")
                print("-" * 110)
                print(f"{'åºå·':<4} {'å…³ç³»ç±»å‹':<12} {'ä¸»ä½“':<15} {'å®¢ä½“':<15} {'æ–¹å‘':<18} {'ç½®ä¿¡åº¦':<8} {'ç±»å‹':<8} {'ä¸Šä¸‹æ–‡'}")
                print("-" * 110)
                for i, relation in enumerate(intra_relations, 1):
                    subject = relation['subject']
                    object_entity = relation['object']
                    confidence = f"{relation['confidence']:.2f}"
                    context = relation['context'][:20] + "..." if len(relation['context']) > 20 else relation['context']
                    direction = relation['direction']
                    rel_type = relation.get('type', 'binary')
                    
                    print(f"{i:<4} {relation['relation']:<12} {subject['text']:<15} {object_entity['text']:<15} {direction:<18} {confidence:<8} {rel_type:<8} {context}")
                    
                    # å¦‚æœæ˜¯ä¸‰å…ƒå…³ç³»ï¼Œæ˜¾ç¤ºè¾…åŠ©å®ä½“
                    if 'auxiliary' in relation:
                        aux_entity = relation['auxiliary']
                        print(f"{'':>50} è¾…åŠ©å®ä½“: {aux_entity['text']} ({aux_entity['type_name']})")
            
            if inter_relations:
                print("\nè·¨å¥å­å…³ç³»:")
                print("-" * 120)
                print(f"{'åºå·':<4} {'å…³ç³»ç±»å‹':<12} {'ä¸»ä½“':<15} {'å®¢ä½“':<15} {'æ–¹å‘':<18} {'ç½®ä¿¡åº¦':<8} {'å¥è·':<6} {'ä¸Šä¸‹æ–‡'}")
                print("-" * 120)
                for i, relation in enumerate(inter_relations, 1):
                    subject = relation['subject']
                    object_entity = relation['object']
                    confidence = f"{relation['confidence']:.2f}"
                    context = relation['context'][:25] + "..." if len(relation['context']) > 25 else relation['context']
                    direction = relation['direction']
                    sentence_dist = relation.get('sentence_distance', 'N/A')
                    
                    print(f"{i:<4} {relation['relation']:<12} {subject['text']:<15} {object_entity['text']:<15} {direction:<18} {confidence:<8} {sentence_dist:<6} {context}")
        else:
            print("  æœªæŠ½å–åˆ°å…³ç³»")
        
        print("\n" + "="*80)
    
    def export_to_json(self, result, filename=None):
        """å¯¼å‡ºç»“æœä¸ºJSONæ ¼å¼"""
        import json
        from datetime import datetime
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"relation_extraction_result_{timestamp}.json"
        
        # æ¸…ç†æ•°æ®ï¼Œç¡®ä¿å¯åºåˆ—åŒ–
        export_data = {
            'timestamp': datetime.now().isoformat(),
            'input': {
                'text': result.get('text', result.get('full_text', '')),
                'text_list': result.get('text_list', []),
                'type': 'multi-sentence' if result.get('text_list') else 'single-sentence'
            },
            'statistics': {
                'total_entities': len(result['entities']),
                'total_relations': len(result['relations']),
                'entity_types': {},
                'relation_types': {}
            },
            'entities': [],
            'relations': []
        }
        
        # å¤„ç†å®ä½“æ•°æ®
        for entity in result['entities']:
            entity_data = {
                'text': entity['text'],
                'type': entity['type'],
                'type_name': entity['type_name'],
                'start': entity['start'],
                'end': entity['end']
            }
            if 'global_start' in entity:
                entity_data['global_start'] = entity['global_start']
                entity_data['global_end'] = entity['global_end']
            if 'sentence_id' in entity:
                entity_data['sentence_id'] = entity['sentence_id']
            
            export_data['entities'].append(entity_data)
            
            # ç»Ÿè®¡å®ä½“ç±»å‹
            entity_type = entity['type_name']
            export_data['statistics']['entity_types'][entity_type] = export_data['statistics']['entity_types'].get(entity_type, 0) + 1
        
        # å¤„ç†å…³ç³»æ•°æ®
        for relation in result['relations']:
            relation_data = {
                'relation': relation['relation'],
                'subject': {
                    'text': relation['subject']['text'],
                    'type': relation['subject']['type'],
                    'type_name': relation['subject']['type_name']
                },
                'object': {
                    'text': relation['object']['text'],
                    'type': relation['object']['type'],
                    'type_name': relation['object']['type_name']
                },
                'confidence': relation['confidence'],
                'context': relation['context'],
                'direction': relation['direction'],
                'type': relation.get('type', 'binary'),
                'scope': relation.get('scope', 'intra-sentence')
            }
            
            if 'auxiliary' in relation:
                relation_data['auxiliary'] = {
                    'text': relation['auxiliary']['text'],
                    'type': relation['auxiliary']['type'],
                    'type_name': relation['auxiliary']['type_name']
                }
            
            if 'sentence_distance' in relation:
                relation_data['sentence_distance'] = relation['sentence_distance']
            
            export_data['relations'].append(relation_data)
            
            # ç»Ÿè®¡å…³ç³»ç±»å‹
            relation_type = relation['relation']
            export_data['statistics']['relation_types'][relation_type] = export_data['statistics']['relation_types'].get(relation_type, 0) + 1
        
        # å†™å…¥æ–‡ä»¶
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            print(f"\nç»“æœå·²å¯¼å‡ºåˆ°: {filename}")
            return filename
        except Exception as e:
            print(f"å¯¼å‡ºå¤±è´¥: {e}")
            return None
    
    def export_to_csv(self, result, filename=None):
        """å¯¼å‡ºå…³ç³»ç»“æœä¸ºCSVæ ¼å¼"""
        import csv
        from datetime import datetime
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"relation_extraction_result_{timestamp}.csv"
        
        try:
            with open(filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                
                # å†™å…¥å¤´éƒ¨
                headers = [
                    'å…³ç³»ç±»å‹', 'ä¸»ä½“æ–‡æœ¬', 'ä¸»ä½“ç±»å‹', 'å®¢ä½“æ–‡æœ¬', 'å®¢ä½“ç±»å‹',
                    'ç½®ä¿¡åº¦', 'æ–¹å‘', 'ç±»å‹', 'èŒƒå›´', 'ä¸Šä¸‹æ–‡'
                ]
                writer.writerow(headers)
                
                # å†™å…¥å…³ç³»æ•°æ®
                for relation in result['relations']:
                    row = [
                        relation['relation'],
                        relation['subject']['text'],
                        relation['subject']['type_name'],
                        relation['object']['text'],
                        relation['object']['type_name'],
                        f"{relation['confidence']:.3f}",
                        relation['direction'],
                        relation.get('type', 'binary'),
                        relation.get('scope', 'intra-sentence'),
                        relation['context']
                    ]
                    writer.writerow(row)
            
            print(f"\nå…³ç³»ç»“æœå·²å¯¼å‡ºåˆ°: {filename}")
            return filename
        except Exception as e:
            print(f"CSVå¯¼å‡ºå¤±è´¥: {e}")
            return None
    
    def generate_statistics_report(self, result):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        entities = result['entities']
        relations = result['relations']
        
        print("\n" + "="*60)
        print("ç»Ÿè®¡æŠ¥å‘Š")
        print("="*60)
        
        # å®ä½“ç»Ÿè®¡
        entity_stats = {}
        for entity in entities:
            entity_type = entity['type_name']
            entity_stats[entity_type] = entity_stats.get(entity_type, 0) + 1
        
        print(f"\nå®ä½“ç»Ÿè®¡ (æ€»è®¡: {len(entities)}ä¸ª):")
        print("-" * 30)
        for entity_type, count in sorted(entity_stats.items()):
            percentage = (count / len(entities)) * 100 if entities else 0
            print(f"{entity_type:<12}: {count:>3}ä¸ª ({percentage:5.1f}%)")
        
        # å…³ç³»ç»Ÿè®¡
        relation_stats = {}
        confidence_stats = {'high': 0, 'medium': 0, 'low': 0}
        type_stats = {'binary': 0, 'ternary': 0}
        scope_stats = {'intra-sentence': 0, 'inter-sentence': 0}
        
        for relation in relations:
            # å…³ç³»ç±»å‹ç»Ÿè®¡
            rel_type = relation['relation']
            relation_stats[rel_type] = relation_stats.get(rel_type, 0) + 1
            
            # ç½®ä¿¡åº¦ç»Ÿè®¡
            confidence = relation['confidence']
            if confidence >= 0.7:
                confidence_stats['high'] += 1
            elif confidence >= 0.5:
                confidence_stats['medium'] += 1
            else:
                confidence_stats['low'] += 1
            
            # ç±»å‹ç»Ÿè®¡
            rel_structure = relation.get('type', 'binary')
            type_stats[rel_structure] += 1
            
            # èŒƒå›´ç»Ÿè®¡
            scope = relation.get('scope', 'intra-sentence')
            scope_stats[scope] += 1
        
        print(f"\nå…³ç³»ç»Ÿè®¡ (æ€»è®¡: {len(relations)}ä¸ª):")
        print("-" * 30)
        for rel_type, count in sorted(relation_stats.items()):
            percentage = (count / len(relations)) * 100 if relations else 0
            print(f"{rel_type:<12}: {count:>3}ä¸ª ({percentage:5.1f}%)")
        
        print(f"\nç½®ä¿¡åº¦åˆ†å¸ƒ:")
        print("-" * 20)
        for level, count in confidence_stats.items():
            percentage = (count / len(relations)) * 100 if relations else 0
            level_name = {'high': 'é«˜(â‰¥0.7)', 'medium': 'ä¸­(0.5-0.7)', 'low': 'ä½(<0.5)'}[level]
            print(f"{level_name:<10}: {count:>3}ä¸ª ({percentage:5.1f}%)")
        
        if any(scope_stats.values()):
            print(f"\nå…³ç³»èŒƒå›´:")
            print("-" * 15)
            for scope, count in scope_stats.items():
                percentage = (count / len(relations)) * 100 if relations else 0
                scope_name = {'intra-sentence': 'å¥å†…', 'inter-sentence': 'è·¨å¥'}[scope]
                print(f"{scope_name:<8}: {count:>3}ä¸ª ({percentage:5.1f}%)")
        
        print("\n" + "="*60)
    
    def interactive_mode(self):
        """äº¤äº’æ¨¡å¼"""
        print("\n" + "="*60)
        print("å¢å¼ºå‹å®ä½“è¯†åˆ«ä¸å…³ç³»æŠ½å–ç³»ç»Ÿ")
        print("="*60)
        print("è¾“å…¥ä¸­æ–‡å¥å­è¿›è¡Œå®ä½“è¯†åˆ«å’Œå…³ç³»æŠ½å–")
        print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("è¾“å…¥ 'help' æŸ¥çœ‹æ”¯æŒçš„åŠŸèƒ½")
        print("è¾“å…¥ 'multi' è¿›å…¥å¤šå¥å­æ¨¡å¼")
        print("è¾“å…¥ 'stats' æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯")
        print("-" * 60)
        
        session_results = []  # ä¿å­˜æœ¬æ¬¡ä¼šè¯çš„æ‰€æœ‰ç»“æœ
        
        while True:
            try:
                user_input = input("\nè¯·è¾“å…¥å¥å­: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    if session_results:
                        save_choice = input("æ˜¯å¦ä¿å­˜æœ¬æ¬¡ä¼šè¯çš„ç»“æœ? (y/n): ").strip().lower()
                        if save_choice in ['y', 'yes', 'æ˜¯']:
                            self._save_session_results(session_results)
                    print("å†è§ï¼")
                    break
                    
                elif user_input.lower() in ['help', 'å¸®åŠ©']:
                    self._show_help()
                    continue
                    
                elif user_input.lower() == 'multi':
                    result = self._multi_sentence_mode()
                    if result:
                        session_results.append(result)
                    continue
                    
                elif user_input.lower() == 'stats':
                    if session_results:
                        self._show_session_stats(session_results)
                    else:
                        print("æš‚æ— ç»Ÿè®¡æ•°æ®ï¼Œè¯·å…ˆè¿›è¡Œå…³ç³»æŠ½å–")
                    continue
                    
                elif not user_input:
                    print("è¯·è¾“å…¥æœ‰æ•ˆçš„å¥å­")
                    continue
                
                # è¿›è¡Œå…³ç³»æŠ½å–
                result = self.extract_relations(user_input)
                session_results.append(result)
                
                # æ˜¾ç¤ºç»“æœ
                self.format_results(result)
                
                # æä¾›å¯¼å‡ºé€‰é¡¹
                export_choice = input("\næ˜¯å¦å¯¼å‡ºç»“æœ? (j=JSON, c=CSV, n=ä¸å¯¼å‡º): ").strip().lower()
                if export_choice == 'j':
                    self.export_to_json(result)
                elif export_choice == 'c':
                    self.export_to_csv(result)
                
            except KeyboardInterrupt:
                print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
                break
            except Exception as e:
                print(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    
    def _show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        print("\næ”¯æŒçš„å…³ç³»ç±»å‹:")
        print("-" * 40)
        for relation_name, pattern_info in self.relation_patterns.items():
            subject_type = self.entity_map.get(pattern_info['subject_type'], pattern_info['subject_type'])
            object_type = self.entity_map.get(pattern_info['object_type'], pattern_info['object_type'])
            if pattern_info.get('is_ternary'):
                aux_type = self.entity_map.get(pattern_info.get('auxiliary_type', ''), '')
                print(f"  {relation_name}: {subject_type} -> {object_type} ({aux_type})")
            else:
                print(f"  {relation_name}: {subject_type} -> {object_type}")
        
        print("\næ”¯æŒçš„å‘½ä»¤:")
        print("-" * 20)
        print("  help/å¸®åŠ© - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
        print("  multi - è¿›å…¥å¤šå¥å­æ¨¡å¼")
        print("  stats - æ˜¾ç¤ºæœ¬æ¬¡ä¼šè¯ç»Ÿè®¡")
        print("  quit/exit/é€€å‡º - é€€å‡ºç¨‹åº")
        
        print("\næ”¯æŒçš„å®ä½“ç±»å‹:")
        print("-" * 20)
        for etype, name in self.entity_map.items():
            print(f"  {etype}: {name}")
    
    def _multi_sentence_mode(self):
        """å¤šå¥å­æ¨¡å¼"""
        print("\nè¿›å…¥å¤šå¥å­æ¨¡å¼ - è¾“å…¥å¤šä¸ªå¥å­ï¼Œç©ºè¡Œç»“æŸ:")
        sentences = []
        
        while True:
            sentence = input(f"å¥å­ {len(sentences)+1}: ").strip()
            if not sentence:
                break
            sentences.append(sentence)
        
        if not sentences:
            print("æœªè¾“å…¥ä»»ä½•å¥å­")
            return None
        
        print(f"\nå¼€å§‹å¤„ç† {len(sentences)} ä¸ªå¥å­...")
        result = self.cross_sentence_relation_extraction(sentences)
        
        # æ˜¾ç¤ºç»“æœ
        self.format_results(result)
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_statistics_report(result)
        
        return result
    
    def _show_session_stats(self, session_results):
        """æ˜¾ç¤ºä¼šè¯ç»Ÿè®¡"""
        total_entities = sum(len(result['entities']) for result in session_results)
        total_relations = sum(len(result['relations']) for result in session_results)
        
        print("\n" + "="*50)
        print("æœ¬æ¬¡ä¼šè¯ç»Ÿè®¡")
        print("="*50)
        print(f"å¤„ç†æ–‡æœ¬æ•°: {len(session_results)}")
        print(f"è¯†åˆ«å®ä½“æ€»æ•°: {total_entities}")
        print(f"æŠ½å–å…³ç³»æ€»æ•°: {total_relations}")
        
        if total_relations > 0:
            avg_confidence = sum(
                sum(rel['confidence'] for rel in result['relations']) 
                for result in session_results
            ) / total_relations
            print(f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
        
        print("="*50)
    
    def _save_session_results(self, session_results):
        """ä¿å­˜ä¼šè¯ç»“æœ"""
        from datetime import datetime
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        combined_result = {
            'session_info': {
                'timestamp': datetime.now().isoformat(),
                'total_texts': len(session_results),
                'session_type': 'interactive'
            },
            'results': session_results
        }
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"session_results_{timestamp}.json"
        
        try:
            import json
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(combined_result, f, ensure_ascii=False, indent=2, default=str)
            print(f"ä¼šè¯ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            print(f"ä¿å­˜å¤±è´¥: {e}")

def main():
    print("å¢å¼ºå‹BERT-NER-CRF å®ä½“è¯†åˆ«ä¸å…³ç³»æŠ½å–ç³»ç»Ÿ")
    print("=" * 60)
    
    # æ¨¡å‹è·¯å¾„
    model_path = "outputs\\cner_output\\bert"
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(os.path.join(model_path, "model.safetensors")):
        print(f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ {os.path.join(model_path, 'model.safetensors')}")
        return
    
    # åˆ›å»ºå…³ç³»æŠ½å–ç³»ç»Ÿ
    extractor = RelationExtractor(model_path)
    
    # æµ‹è¯•ç¤ºä¾‹
    print("\nå…ˆç”¨ç¤ºä¾‹æµ‹è¯•ç³»ç»Ÿ:")
    test_examples = [
        "å¼ ä¸‰æ˜¯è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œåœ¨åŒ—äº¬é˜¿é‡Œå·´å·´å·¥ä½œï¼Œä½åœ¨æœé˜³åŒºï¼Œç”µè¯æ˜¯13800138000ã€‚",
        "ææ•™æˆæ¯•ä¸šäºæ¸…åå¤§å­¦è®¡ç®—æœºä¸“ä¸šï¼Œç°åœ¨å¦é—¨æ³›åé›†å›¢æ‹…ä»»æŠ€æœ¯æ€»ç›‘ã€‚",
        "ç‹å°æ˜åœ¨ä¸Šæµ·å¾®è½¯å…¬å¸åšäº§å“ç»ç†ï¼Œå®¶ä½æµ¦ä¸œæ–°åŒºã€‚",
        # æ–°å¢å¤æ‚ç¤ºä¾‹
        "é™ˆåšå£«æ˜¯æ±‰æ—äººï¼Œåœ¨ä¸­ç§‘é™¢è®¡ç®—æ‰€ä»äº‹äººå·¥æ™ºèƒ½ç ”ç©¶ï¼ŒåŒæ—¶æ‹…ä»»å®éªŒå®¤ä¸»ä»»ã€‚",
        "è…¾è®¯å…¬å¸ä½äºæ·±åœ³ï¼Œå…¶ä¸‹å±çš„å¾®ä¿¡äº‹ä¸šéƒ¨åœ¨å¹¿å·è®¾æœ‰åŠå…¬å®¤ã€‚"
    ]
    
    for i, example in enumerate(test_examples, 1):
        print(f"\n{'='*25} ç¤ºä¾‹ {i} {'='*25}")
        result = extractor.extract_relations(example)
        extractor.format_results(result)
        if i <= 3:  # ä¸ºå‰3ä¸ªç¤ºä¾‹ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            extractor.generate_statistics_report(result)
    
    # æµ‹è¯•å¤šå¥å­åŠŸèƒ½
    print(f"\n{'='*30} å¤šå¥å­æµ‹è¯• {'='*30}")
    multi_sentences = [
        "å¼ ä¸‰åœ¨é˜¿é‡Œå·´å·´å·¥ä½œã€‚",
        "ä»–æ‹…ä»»é«˜çº§è½¯ä»¶å·¥ç¨‹å¸ˆèŒä½ã€‚",
        "å®¶ä½åœ¨æ­å·è¥¿æ¹–åŒºã€‚"
    ]
    
    multi_result = extractor.cross_sentence_relation_extraction(multi_sentences)
    extractor.format_results(multi_result)
    extractor.generate_statistics_report(multi_result)
    
    # å¯¼å‡ºç¤ºä¾‹
    print(f"\n{'='*25} å¯¼å‡ºåŠŸèƒ½æµ‹è¯• {'='*25}")
    json_file = extractor.export_to_json(multi_result)
    csv_file = extractor.export_to_csv(multi_result)
    
    # è¿›å…¥äº¤äº’æ¨¡å¼
    extractor.interactive_mode()

if __name__ == "__main__":
    main()